[{"title":"全景分割论文跟踪","url":"/2020/12/19/20201219-papers-003/","content":"\n正好在做全景分割任务的文献调研，顺手把调研结果整理了一下（目前截至2020年12月19日），以及其他涉及到全景分割的论文。项目里按照发表刊物分类，格式是: \n\n**[论文名称 | 发表刊物 | 论文地址 | 开源代码 | 博客解读]**\n\n后面会每周进行更新，欢迎大家关注。\n\n项目地址：https://github.com/YvesXu/panoptic-segmentation-paper-list\n\n目前一共统计了60篇论文，其中针对全景分割任务有42篇，已发表24篇，未发表18篇(2020年占14篇)，等接下来的结果出来，估计这里面也有很多会变成已发表。\n\n项目中论文排序优先级：\n\n1. 已经发表的会议论文\n2. 已发表的其他论文\n3. 未发表的论文\n\n同级别的论文按照年份，以及arXiv编号倒序排列，目前benchmark还没有更新。\n\n","tags":["深度学习","全景分割"],"categories":["papers"]},{"title":"【全景分割论文阅读002 AUNet】 Attention-guided Unified Network for Panoptic Segmentation","url":"/2020/12/01/20201201-papers-002/","content":"\n本文在Mask R-CNN的基础上加入了语义分支，实现了单个网络的全景分割架构。运用注意力模块，将前景特征融入背景特征中，ResNeXt-152-FPN版本在COCO 2018 test-dev上的PQ指标达到46.5；ResNet-101-FPN在CityScapes上的PQ指标为59.0。\n\n<!-- more -->\n\n#### 0 基本情况\n论文标题： Attention-guided Unified Network for Panoptic Segmentation\n收录情况：CVPR 2019\n原文地址：https://arxiv.org/pdf/1812.03904.pdf\n\n#### 1 网络架构\n整个网络由4个模块组成: ResNet + FPN + foreground Branch + background Branch。两个分支共享前面的特征提取部分，在前景分支上，采用二阶段的实例分割方法，背景分支则是选用了一个轻量的语义分割head。另外再引入两个注意力模块，来实现前景背景之间的融合。\n\nPAM用于目标级别的注意力，对RPN特征与语义分割特征逐层(来自FPN)融合；MAM则是融合roi模块与语义分割特征(concat后)。\n![AUNet网络结构](http://hexotuchuang.oss-cn-beijing.aliyuncs.com/blog_init/post/papers/20201201_aunet_1_architecture.png)\n\n#### 2 模块设计\n由于前景和背景是互补的，因此文章将前景特征通过注意力机制，融入背景特征，提供更多的上下文线索。\n\n##### 2.1 PAM\n文中提出RPN作为一个proposal的二分类器，包含了大量的背景信息。因此提出PAM(Proposal Attention Module)，用来辅助语义分割。模块如下图所示，可以分为两部分。前半部分是背景增强模块，通过`1-sigmoid`操作，抑制PRN中的前景特征权重，进行背景增强；后半部分是沿用`SENet`的思路，对语义特征不同层之间进行权重分配。（这部分未单独证明）\n\n通过两部分共同作用，实现目标级别的背景特征增强。同时，由于反向传播，可以增强前景特征，提升前景学习的效果。\n![PAM](http://hexotuchuang.oss-cn-beijing.aliyuncs.com/blog_init/post/papers/20201201_aunet_2_pam.png)\n##### 2.2 MAM\n由于PAM是目标级别的注意力，文中为了得到更加精细背景，引入基于前景mask的注意力模块MAM，提供像素级的注意力。\n文中提出了RoIUpsample，从$m\\times m $的mask中提出原始尺度的特征，主要使用了反向双线性插值，近似于RoIAlign的逆运算。\n整体结构如下图图示，运用RoIUpsample，将$m\\times m $的mask生成与FPN结构类似的多尺度特征。通过上采样将不同层级的mask进行叠加，后续的结构与PAM一致。\n![MAM](http://hexotuchuang.oss-cn-beijing.aliyuncs.com/blog_init/post/papers/20201201_aunet_3_mam.png)\n\n#### 3 实验细节\n##### 3.1 Loss 设计\n\n$L = \\lambda_1L_{RPN} + \\lambda_2L_{RCNN}+\\lambda_3L_{Mask} + \\lambda_4L_{Seg}$\n\n$L_{RPN}$, $L_{RCNN}$, $L_{Mask}$, $L_{Seg}$分别对应RPN, RCNN, 实例分割，语义分割四项loss，在COCO上，权重为${1, 1, 1, 0.3}$；CityScapes上权重为{1, 0.75, 1, 1}\n\n##### 3.2 训练细节\n- 网络超参配置与Mask R-CNN一致，backbone在ImageNet预训练。batch_size为2，采用SGD优化，weight decay =4e-5，动量momentum=0.9\n- COCO数据集上，一共18个epoch，前13个epoch学习率为0.02，在第15和第18个epoch时，除以10。输入图像短边resize为600。\n- CityScapes数据集上，学习率初始化为0.01，在第68和88个epoch时，除以10。图像随机翻转，缩放为0.5到2.0倍。\n##### 3.3 推理过程\n\n针对物体重叠问题，根据置信度得分来解决冲突，同时考虑物体间的关系。在前景与背景冲突的情况下，以前景为准。\n\n##### 3.4 实验结果\n各模块的增益效果，可以看出两个模块相较于端到端的算法都有所提升。在COCO数据集上，不采用任何数据增强，PQ指标为39.6。\n\n![消融实验](http://hexotuchuang.oss-cn-beijing.aliyuncs.com/blog_init/post/papers/20201201_aunet_4_result.png)\n\n值得注意的一点是MAM的指标优化不如PAM，文中的`猜测`是由于Mask生成特征缺乏上下文信息。\n基于ResNet-101的AUNet，在COCO的test-dev上的PQ值为45.2，在Cityscapes上的指标为59.0（在分类时，选用了19类，而没有像COCO数据集那样，都分为`things`，）。详细指标见下图。\n![COCO test-dev结果](http://hexotuchuang.oss-cn-beijing.aliyuncs.com/blog_init/post/papers/20201201_aunet_6_coco_res.png)\n![CityScapes val结果](http://hexotuchuang.oss-cn-beijing.aliyuncs.com/blog_init/post/papers/20201201_aunet_7_cityscapes_res.png)\n\n#### 4 其他内容\n- 问：前景和背景互补，那么为什么可以利用前景特征来促进背景？\n    - 答：根据注意力模块的表现来看（见下图），会对前景特征赋予低权值，背景特征赋予高权值，可以认为是对背景特征的一个增强。\n在PAM中，注意力模块作用于语义特征前，进行了1-sigmoid(x)的操作，可以认为是抑制前景，增强背景。\n![注意力激活](http://hexotuchuang.oss-cn-beijing.aliyuncs.com/blog_init/post/papers/20201201_aunet_5_visual.png)\n\n文章未说明内容：\n- PAM中间的降维过程，从FPN的256维度，到中间维度，再到1维，这里的中间维度是多少？\n\n思考：\n- MAM如果对应回特征层面，而不是插值，是否能够带来更大的性能提升？\n- 能否将背景特征融入前景中去？","tags":["深度学习","全景分割"],"categories":["papers"]},{"title":"detectron2 源码阅读系列 03 内置数据集","url":"/2020/11/30/20201130-detectron2-03-builtin-datasets/","content":"\n### 1  内置数据集下载\n\ndetecrton2内置了一部分数据集的配置，包括：COCO的instance/ keypoint detectron，LVIS实例分割，citytscapes, Pascal VOC, ADE20k，以及Panoptic FPN 全景分割格式。\n\n<!-- more -->\n\n这里以COCO2017和citytscapes的全景分割格式作为例子。\nCOCO下载地址：https://cocodataset.org/#download\ncityscapes(需要登录)：https://www.cityscapes-dataset.com/downloads/\n\n如果下载速度慢的话，可以选择graviti（需要登陆）。\nCOCO：https://www.graviti.cn/open-datasets/dataset-detail/COCO\ncityscapes：https://www.graviti.cn/open-datasets/dataset-detail/CityScapes\n\n### 2 文件结构\n\n解压后，整个的文件结构如下，需要通过其他命令来生成：\n```bash\ndetectron2/\n|---datasets/\n    |---cityscapes/\n        |---leftImg8bit/\n            |---train/\n            |---val/\n            |---test/\n        |---gtFine/\n            |---cityscapes_panoptic_train.json(生成)\n            |---cityscapes_panoptic_train/(生成)\n            |---cityscapes_panoptic_val.json(生成)\n            |---cityscapes_panoptic_val/(生成)\n            |---cityscapes_panoptic_test.json(生成)\n            |---cityscapes_panoptic_test/(生成)\n            |---train/\n                |---aachen/\n                    |---*color.png\n                    |---*instanceIds.png\n                    |---*labelIds.png\n                    |---*labelTrainIds.png\n                    |---*polygons.json\n                    ...\n                ...\n            |---val/\n            |---test/\n        |---cityscapesScripts-master/\n    |---coco/\n        |---annotations/\n            |---instances_train2017.json\n            |---instances_val2017.json\n            |---person_keypoints_train2017.json\n            |---person_keypoints_val2017.json\n            |---captions_train2017.json\n            |---captions_val2017.json\n            |---panoptic_train2017.json\n            |---panoptic_val2017.json\n        |---panoptic_stuff_train2017/(生成)\n        |---panoptic_stuff_val2017/(生成)\n        |---panoptic_train2017/\n        |---panoptic_val2017/\n        |---train2017/\n        |---val2017/\n...\n```\n### 3 数据生成\n#### 3.1 COCO 的PanopticFPN\n\n在`datasets/`文件下，运行以下命令，生成COCO 的全景分割数据：\n\n```python\npip install git+https://github.com/cocodataset/panopticapi.git\npython prepare_panoptic_fpn.py\n```\n\n在`datasets/cityscapes/cityscapesScripts-master`文件夹中，运行以下命令，生成CityScapes 的全景分割数据：\n```python\npip install git+https://github.com/mcordts/cityscapesScripts.git\nCITYSCAPES_DATASET=/path/to/abovementioned/cityscapes python cityscapesscripts/preparation/createTrainIdLabelImgs.py\nCITYSCAPES_DATASET=/path/to/abovementioned/cityscapes python cityscapesscripts/preparation/createPanopticImgs.py\n```\n\n最后整体的目录就和上面一样。","tags":["深度学习","pytorch","源码阅读","目标检测","框架学习"],"categories":["codes"]},{"title":"【全景分割论文阅读001 Panoptic FPN】 Panoptic Feature Pyramid Networks","url":"/2020/11/11/20201111-papers-001/","content":"\n本文通过在Mask R-CNN的基础上加入了语义分支，实现了单个网络的全景分割架构，COCO测试集上的PQ指标达到40.9，成为全景分割的baseline。\n\n<!-- more -->\n\n#### 0 基本情况\n论文标题： Panoptic Feature Pyramid Networks\n收录情况：CVPR 2019\n原文地址：https://arxiv.org/pdf/1901.02446.pdf\n\n#### 1 网络架构\n整个网络由4个模块组成: ResNet + FPN + Instance Segmentation Branch + Semantic Segmentation Branch。 从下图中可以看出，网络采用了ResNet + FPN的结构，实例分割选择了Mask R-CNN的结构，语义分割则是在FPN的结构上进行多尺度信息融合后得到的结果。\n\n整个网络的精度基本上等同于两个单独训练的FPN网络，但是内存和计算量只有一半。即在同等计算下，一个联合的网络得到了两个独立网络的效果。\n\n![网络结构](http://hexotuchuang.oss-cn-beijing.aliyuncs.com/blog_init/post/papers/20201123_panoptic_fpn_1_architecture.png)\n\nMask R-CNN部分不再赘述，论文中详细介绍了语义分支的模块设计，如下图。 对于FPN提取出来的特征[P2, P3, P4, P5]，统一放大至1/4尺度后，进行累加，最后再通过卷积得到语义分割结果。图中的`conv->2x`包含了[`3×3`卷积 + `group norm` + `ReLu` + `2× bilinear upsampling`]，最后将这四层特征逐元素相加，并通过`1×1`卷积和`4× bilinear upsampling` + `softmax`，得到了最后语义分割的结果。\n\n注意，这里的语义分割只分割了背景，前景统一标注成`other`类。\n\n![语义分支](http://hexotuchuang.oss-cn-beijing.aliyuncs.com/blog_init/post/papers/20201123_panoptic_fpn_2_seghead.png)\n\n  文章中还下图中不同的语义分支结构的计算量和效率，并进行了对比。\n\n![语义分支不同结构](http://hexotuchuang.oss-cn-beijing.aliyuncs.com/blog_init/post/papers/20201123_panoptic_fpn_4_seghead2.png)\n\n\n#### 2 推理过程\n\n  这里主要解决了双分支可能造成的冲突问题：\n  - 实例间的重叠问题：以置信度高的为准\n  - 实例分割和语义分割的重叠：以实例分割为准\n  - 移除了语义分割中的`other`信息（前景），或者面积小于阈值的部分\n\n#### 3 实验细节\n\n##### 3.1 Loss 设计\n  实验的Loss由两大部分组成：$L_i$实例分割Loss + $L_s$语义分割Loss；\n\n  其中，$L_i = L_c + L_b + L_m$，分别对应类别class，检测框bounding box，掩膜mask三个Loss。前两项用RoIs的数量来归一化，最后一项用前景RoIs的数量来归一化。\n\n  $L_s$的语义分割Loss则是用标记像素数来归一化。\n\n  整体的Loss是二者的加权求和：$L = \\lambda_i(L_c+L_b+L_m)+\\lambda_sL_s$，最后整体训练时候，$\\lambda_i$和$\\lambda_s$从${0.5, 0.75, 1}$中选，确保结果不会产生偏差。\n\n##### 3.2 训练细节\n  - COCO数据集上，采用默认的Mask R-CNN 训练，尺度放缩确保短边落在$[640, 800]$之间\n  - Cityscape数据集上，尺度放缩在0.5倍到2.0倍之间，训练65000次迭代，学习率0.01，在40000和55000次衰减10倍；数据增强上采用了颜色增强和裁切。\n  - 预训练时，采用的是在ImageNet上训完，带有BN的ResNet/ResNeXt；fine-tune时，将BN换成affineChannel，可以加速收敛，可能提高性能。\n\n##### 3.3 实验结果\n\n基于ResNet-101的Panoptic FPN，在COCO的test-dev上的PQ值为40.9，在Cityscapes上的指标为58.1。\n![实验结果](http://hexotuchuang.oss-cn-beijing.aliyuncs.com/blog_init/post/papers/20201123_panoptic_fpn_5_result.png)\n\n\n#### 4 其他内容\n- 问：FPN是为了解决目标检测在多尺度上的精度问题，为什么语义头部可以加入？\n    - 答：语义分割需要(1)高分辨率来获取细节信息；(2) 编码足够丰富的语义；(3)多尺度信息来获得多种分辨率；而FPN的输出符合这些条件。（下表的实验结果也说明了这一点）\n    \n    \n    ![语义分支训练结果](http://hexotuchuang.oss-cn-beijing.aliyuncs.com/blog_init/post/papers/20201123_panoptic_fpn_6_seghead3.png)\n\n文章未说明内容：\n\n- 语义分支的输入是FPN 256维度特征，但是上采样到1/4尺度后是128维，是在什么时候进行的降维操作。\n\n- 模型loss的$\\lambda_i$和$\\lambda_s$分别是多少。\n\n","tags":["深度学习","全景分割"],"categories":["papers"]},{"title":"机器学习实战 读书笔记 01 第一章","url":"/2020/11/07/20201107-skt-01/","content":"\n#### 0. 前言\n这个系列是AI“四大名著”之一《机器学习实战：基于 Scikit-Learn、Keras 和 TensorFlow（第二版）》的读书笔记系列，主要记录阅读过程中的知识点和一些思考。\n\n#### 1. 第1章 机器学习概览\n\n##### 1.1 机器学习的定义\n    一个计算机程序利用经验E来学习任务T，性能是P，如果针对任务T的性能P随着经验E不断增长，则称为机器学习。——Tom Mitchell 1997\n    感觉这个说法也出现在了很多其他的教材里，也十分的工程化。就是针对任务T，通过经验E，寻求性能P的提升。\n\n<!-- more -->\n\n##### 1.2 为什么使用机器学习\n    与这个问题相对应的是：什么样的问题适合用机器学习来解决。主要包括以下几种：\n    - 有解决方案的问题，但是可以通过机器学习算法来简化代码，或提升性能（书中给出的例子是垃圾邮件过滤，传统方法需要手工设计大量的规则来加以判断，且难以维护）\n    - 传统方法难以解决的复杂问题(比如同音单词识别)\n    - 环境有波动，适应新数据\n    - 洞察复杂问题和大量数据，挖掘数据中的规律\n\n##### 1.3 机器学习的类型\n<table>\n    <tr>\n        <th>分类依据</th>\n        <th>类别</th>\n        <th>特点</th>\n        <th>具体算法或备注</th>\n    </tr>\n    <tr>\n        <td rowspan=\"4\">有无监督信号</td>\n        <td>监督学习</td>\n        <td>提供了包含解决方案(标签/label/ground truth)的训练集，主要包含分类和回归两大类</td>\n        <td><li>k-近邻</li><li>线性回归</li><li>逻辑回归</li><li>SVM</li><li>决策树和随机森林</li></td>\n    </tr>\n    <tr>\n        <td>无监督学习</td>\n        <td>所有训练数据均未标记，主要包含聚类、异常检测和新颖性检测、可视化和降维、关联规则学习</td>\n        <td><li>聚类：k-means、DBSCAN、分层聚类分析HCA</li>\n        <li>异常检测和新颖性检测：单类SVM、孤立森林</li>\n        <li>可视化和降维：主成分分析(PCA)、核主成分分析、局部线性嵌入(LLE)、t-分布随机近邻嵌入(t-SNE)</li>\n        <li>关联规则学习：Apriori、Eclat</li></td>\n    </tr>\n    <tr>\n        <td>半监督学习</td>\n        <td>少量有标记数据和大量未标记数据，通常是有监督和无监督算法的组合</td>\n        <td><li>深度信念网络DBN</li></td>\n    </tr>\n    <tr>\n        <td>强化学习</td>\n        <td>通过观察环境，做出选择，执行动作，得到回报（或惩罚）</td>\n        <td><li>Q-learning</li><li>Sarsa</li></td>\n    </tr>\n    <tr>\n        <td rowspan=\"2\">是否进行增量学习</td>\n        <td>批量学习</td>\n        <td>离线进行训练后，部署至生存环境，因此是在完整数据集上训练；遇到新的数据后，需要在新的完整数据集上重新训练，停用旧系统，启用新系统。 </td>\n        <td>缺点：耗时占资源，每次训练需要完整数据集训练几个小时，且完整数据集训练需要大量计算资源</td>\n    </tr>\n    <tr>\n        <td>在线学习</td>\n        <td>可以根据新数据进入不断学习调整模型，每次只学习新数据，完成后即可丢弃数据</td>\n        <td><li>核外学习：针对超大数据集，存储在不同计算机上，每次只加载部分数据进行学习，直至完成整个数据集训练，这个过程通常是<font color=#0087BB>离线</font>的</li><li>在线学习: 由于模型需要根据新数据进行调整，则需要设置学习率，来决定适应新数据（忘记旧数据）的速度，同时也要求<font color=#0087BB>监控异常数据</font></li></td>\n    </tr>\n    <tr>\n        <td rowspan=\"2\">模型泛化模式</td>\n        <td>基于实例的学习</td>\n        <td>记住学过的所有数据，将新数据与已学过的数据进行比对，根据相似度来给出预测结果</td>\n        <td>需要选择多少个最近邻的实例</td>\n    </tr>\n    <tr>\n        <td>基于模型的学习</td>\n        <td>通过数据构建相应的模型（找出数据中的规律），然后根据模型对新数据进行预测</td>\n        <td>超参数：模型的复杂度；参数：训练过程的求解结果；目标函数：训练过程中的优化目标</td>\n    </tr>\n</table>\n\n\n##### 1.4 机器学习的主要挑战及解决方法\n- 数据问题\n    - 数据数量不足：构建更大的数据集\n    - 数据不具有代表性（采样偏差）：改进采样方法\n    - 低质量数据（噪声、错误和异常值等）：丢弃异常值，忽略/修复缺失的部分特征\n    - 无关特征：特征选择，特征提取，收集新数据\n- 训练过程\n    - 过拟合：选择更小参数的模型/减少数据中的属性数量/约束模型（正则化）；收集更多数据；减少数据中的噪声\n    - 欠拟合：选择更多参数的模型；提供更好的特征集（更加容易学习）；减少模型中的约束    \n\n\n##### 1.5 测试与验证\n- 测试集：为了确保模型在生产环境中的可靠性，将数据集分割成训练集和测试集，用测试集来评估模型。\n- 验证集(dev/val set)：如果选出测试集表现最好的模型，则可能是对测试集部分的过拟合，实际环境中表现不如预期。因此从训练集中再分割出一部分作为验证集，用验证集来挑选模型，测试集来评估模型。\n- 训练开发集合(train-dev set)：在数据缺乏的情况下，可以采用其他相近数据来代替，但是由于数据来源不同，可能出现训练集测试集<font color=#0087BB>数据不匹配(mismatch)</font>的情况，因此将真实场景数据划分为验证集和测试集，而训练集数据划分为训练集和训练开发集（验证集）：\n    - 如果在训练开发集表现较差，代表模型过拟合；\n    - 如果在训练开发集表现较好，而验证集表现差，则问题来自数据不匹配，需要对数据进行处理。 ","tags":["读书笔记","机器学习"],"categories":["reading_notes"]},{"title":"Python学习笔记01 \"is\" 和 \"==\" 的区别","url":"/2020/10/12/20201012-python-01/","content":"\n\n#### 1. `is` 和 \"==\" 的概念\n`==`比较的是两个对象的“值“是否相等\n`is`比较的是两个对象在内存中的地址是否相同（值相等，内存一致，可以理解为`id(a) == id(b)`）\n\n\n**代码示例**\n```python\na = 19260817\nb = 19260817\nc = a \nprint(a == b)\n# 数值一致, 返回True\nprint(a is b)\n# 地址不一致, 返回False\nprint(a == c)\n# 数值一致, 返回True\nprint(a is c)\n# 地址一致, 返回True\n```\n<!-- more -->\n#### 2. 其他关于`is`的注意事项：\n- 为了提升性能, python将<font color=#0087BB>$[-5, 256]$</font>区间内的整数放在了`small_ints`中,需要用到的时候直接从里面取值, 而不是创造新的对象, 因此区间内的对象只要数值相同, 他们的地址也相同。\n- 类似的, 为了提高字符串利用效率, python使用了intern(字符串驻留)机制, 将<font color=#0087BB>长度不超过20, 且仅由下划线、数字、字母组成的字符串</font>以`{string: id(string)}`的字典形式存储, 相同字符串使用同一个地址。(在`Pycharm`和`VS code`等IDE中, 只要字符串<font color=#0087BB>长度不超过20</font>, 就会采用上述形式存储。)\n- 和`None`比较时使用`is`, `None`也是有专门的地址, 因此只要变量是`None`, 均是同一个地址。\n\n**代码示例**\n```python    \na = 52\nb = 52\nprint(a is b)\n# 数值在[-5, 256]区间内, 返回True\n\na = 520\nb = 520\nprint(a is b)\n# 数值不在[-5, 256]区间内, 返回False\n\na = \"ab\"\nb = \"ab\"\nprint(a is b)\n# 满足`intern`机制要求, 返回True\n\na = \"a b\"\nb = \"a b\"\nprint(a is b)\n# 不一定满足`intern`机制要求, 在python终端中返回False, Pycharm中返回True\n\na = \"\"\nprint(a is None)\n# a是字符串对象, 有自己的地址, 返回False\n\nb = None\nprint(b is None)\n# 返回True\n```\n\n参考资料：\nhttps://blog.csdn.net/qq_26442553/article/details/82195061\nhttps://www.cnblogs.com/greatfish/p/6045088.html","tags":["python"],"categories":["techs"]},{"title":"算法学习001  树的概念与常见算法 01","url":"/2020/09/28/20200928-alg-01-tree-01/","content":"\n### 1. 树的定义\n>树(Tree)是$n(n\\ge0)$个结点(node)的有限集。在任意一个非空树中：\n>1. 有且只有一个特定的称为根(Root)的结点；\n>2. 当$n > 1$时，其余结点可分为$m(m>0)$个互不相交的有限集，每个集合本身又是一棵树，称为根节点的子树。$^{[1]}$\n\n以下图为例，$1$ 号结点为根节点，其余结点可以划分为$3$ 个互不相交的集合，每个集合都是一棵树，分别为$[2, 5]$, $[3, 6, 7]$, $[4, 8, 9, 10, 11]$。\n\n\n![树](http://hexotuchuang.oss-cn-beijing.aliyuncs.com/blog_init/post/alg/20200928_tree_01.png)\n\n这棵树可以理解为带有多个$next$指针的链表，例如$3$ 号结点后面有$2$个$next$指针，分别指向结点$6$和结点$7$。\n\n如果树中每个结点的子树不超过$2$棵，并且$2$棵子树有左右之分，则把这种树称为二叉树。为了方便讨论，同时考虑到大量的算法题也是基于也是针对后面以二叉树为主，因此后续以二叉树作为讨论对象。\n\n<!-- more -->\n\n在`LeetCode`中，二叉树的`C++` 和 `Python`结构如下：\n```C++\n/* C++ 下的二叉树结构体*/\nstruct TreeNode\n{\n    int val;\n    TreeNode *left;\n    TreeNode *right;\n    TreeNode() : val(0), left(nullptr), right(nullptr){}\n    TreeNode(int x) : val(x), left(nullptr), right(nullptr){}\n    TreeNode(int x, TreeNode *left, TreeNode *right) : val(x), left(left), right(right){}\n};\n```\n\n```python\n# python下的二叉树类\nclass TreeNode:\n    def __init__(self, val=0, left=None, right=None):\n        self.val = val\n        self.left = left\n        self.right = right\n```\n这里也画了二叉树的结构，左右子树各一个指针，指向自己的子树。这个结构本身就具有递归的性质，因此很多相应的问题可以用递归的思路来解决。\n![二叉树](http://hexotuchuang.oss-cn-beijing.aliyuncs.com/blog_init/post/alg/20200928_tree_02.png)\n\n### 2. 二叉树的性质\n1. 二叉树第$i$层最多有$2^{i-1}$个结点（根节点记为第一层，即$i\\geq1$）\n2. 高度为$k$的二叉树，最多拥有$2^k-1$个结点($k\\geq1$,计算过程：$num_{node}\\leq1+2+4+...+2^{k-1}=2^k-1$  )\n3. 根据性质2可以倒推，拥有$n$个结点的二叉树，高度最少为$log_2(n+1)$\n4. 在任意二叉树下，一共有$n$个结点，$n-1$条边，若叶子结点个数记为$n_0$，有一个孩子的结点(对应一条边)个数为$n_1$，两个孩子结点(对应两条边)的个数记为$n_2$，则可以得出$n_0+n_1+n_2(结点数量) = 2\\times n_2+n_1(边的数量)+1$，整理可以得到$n_0 = n_2 + 1$，主要会在选择题中考察。\n![二叉树结构](http://hexotuchuang.oss-cn-beijing.aliyuncs.com/blog_init/post/alg/20200928_tree_03.png)\n\n### 3. 树的遍历\n\n![二叉树遍历](http://hexotuchuang.oss-cn-beijing.aliyuncs.com/blog_init/post/alg/20200928_tree_04.png)\n\n[1]严蔚敏, 吴伟民. 清华大学计算机系列教材 :数据结构[M]// 清华大学计算机系列教材 ：数据结构. 清华大学出版社, 2011.","tags":["树","算法"],"categories":["algorithms"]},{"title":"detectron2 源码阅读系列 02 Demo.py","url":"/2020/09/10/20200911-detectron2-02-demo/","content":"\n上一篇通过`demo.py`运行得到了一个检测的结果，这次深入到demo文件夹中，分析整体框架的搭建。该文件夹一共有两个文件，分别是`demo.py` 和`predictor.py`，核心功能就是通过获取运行参数中的输入，进行结果预测，并输出可视化结果。\n\n`demo.py`的调用关系如下图所示\n\n![demo结构](http://hexotuchuang.oss-cn-beijing.aliyuncs.com/blog_init/post/detectron2/20200911_demo_frame.png)\n\n<!-- more -->\n\n### 1. demo.py 文件概览\n\n文件主要包含三个函数\n\n- 主函数\n- `get_parser()`\n- `setup_cfg(args)`\n\n#### 1.1 主函数\n\n总体流程\n\n1. `multiprocessing.set_start_method()`设置多线程操作\n2. 自定义的`get_parser()` 函数获取参数，存入`args` \n3. 自定义的`setup_logger()`函数设置日志文件\n4. 使用`logger.info()` 记录参数信息\n5. 自定义的`setup_cfg(arg)` 将获取的参数进行设置，存入`cfg`\n6. 创建<font color=#0087BB>`VisualizationDemo`</font>类的实例为<font color=#0087BB>`demo`</font>\n7. 如果输入是图像`(args.input)`\n   1. 如果输入时单张图片，确认指定路径下的文件存在\n   2. 遍历`args.input`:\n      1. 使用`read_image()`读取路径下的图片\n      2. 记录开始时间\n      3. 调用<font color=#0087BB>`demo.run_on_image`</font>得到预测结果`predictions`和可视化结果`visualized_output`，并记录日志信息（`predictions`预测到多少实例，预测花费时间）\n      4. 如果需要保存结果，则将可视化结果进行保存至`args.output`\n      5. 否则利用窗口进行展示，按`esc`键退出\n8. 如果输入是网络摄像头`(args.webcam)`:\n   1. 确认不包含图片输入`args.input`，且没有输出参数`args.output`\n   2. 设置摄像头为`cam`，并在上面运行<font color=#0087BB>`demo.run_on_video`</font>\n   3. 针对每帧图片的预测结果，窗口显示1ms，按`esc`键退出，退出后释放网络摄像头，关闭所有窗口\n9. 如果输入是视频`(args.video_input)`\n   1. 将视频读进`video`，获取视频的尺寸信息`width`和`height`, 帧率`frames_per_second`，总帧数`num_frams`，\n   2. 在`video`上运行<font color=#0087BB>`demo.run_on_video`</font>，如果需要输出，则写入`output_file`，否则通过窗口显示结果，按`esc`键退出。运行后释放输入输出视频，关闭所有窗口。\n\n\n####  1.2 参数解析的`get_parser()` 函数\n\n这个函数的核心功能是利用系统的`argparse`模块进行参数解析，主要创建`ArgumentParser`对象，调用`.add_argument()`方法定义参数，使用`.parse_args()`进行参数解析三个步骤。第三个步骤在主函数里面调用，因此这个函数核心代码就只包含对象创建以及添加参数，最后返回`ArgumentParser`对象。\n\n```python\ndef get_parser():\n    parser = argparse.ArgumentParser()\n    parser.add_argument()\n```\n\n这里添加的参数有\n- `--config-file` 配置文件位置\n- `--webcam` 网络摄像头\n- `--video-input` 视频路径\n- `--input` 输入图片路径\n- `--output` 可视化结果输出位置，如果为赋值，则通过窗口显示\n- `--confidence-threshold` 显示实例预测的最小阈值\n- `--opts` 其他可选配置\n\n####  1.3 `setup_cfg(args)`函数\n\n前面`get_parser()` 函数返回的是一个参数解析器，主函数调用`.parse_args()`进行参数解析后得到`args`是一个`namespace`，指定了对应的配置文件，还需要将配置文件的参数进行解析。\n\n首先构建了一个默认的`CfgNode`对象，主要用来存放基本的网络参数配置，通过调用`.merge_from_file()` 和 `.merge_from_list()`两个方法解析`args`的指定的配置文件，对默认数值进行更新。\n\n***\n\n### 2. `Predictor.py`文件概览\n\n从上面的总体流程中可以看出，`demo.py`的核心功能是通过<font color =#0087BB>`VisualizationDemo`</font> 类实现的，这个类在同目录下的`predictor.py` 文件中。这个文件内部主要包含两个类：\n- `VisualizationDemo`\n\n- `AsyncPredictor`  \n\n\n#### 2.1 `VisualizationDemo` 类\n\n内部定义了以下四个函数：\n- `__init__()`\n- `run_on_image()`\n- `run_on_video()`\n- `_frame_from_video()`\n\n\t##### `__init__()`\n\t定义了预测参数，并根据并行处理需求将`predictor`设置为默认预测器`DefaultPredictor`或者并行预测器`AsyncPredictor`。\n\n\t##### `run_on_image(self, image)`\n\n\t- 输入：图像（H, W, C）图像模式 (BGR)\n\t- 输出：网络输出结果`predictions`, 可视化结果`vis_output`，可视化结果是`Visualizer`对象。\n\n\t##### `_frame_from_video(self, video)`\n\t获取视频帧\n\n\t##### `run_on_video(self, video)`\n\t调用`_frame_from_video` 方法获取视频帧，对每一帧进行预测，并通过内置的`process_predictions`方法，创建`Visualizer`对象，对结果进行展示。\n\n\t#### `AsyncPredictor`类\n\t主要是针对多GPU预测进行了处理，每个GPU上构建一个默认预测器`DefaultPredictor`，分别进行预测。\n\n### 3. `DefaultPredictor` 类\n\n不论是单GPU，还是多GPU，最后的预测器都调用了`DefaultPredictor` 类，该类位于`detectron2\\engine\\defaults.py`中，内部只有初始化函数`__init__`和回调函数`__call__`，核心功能是通过给定参数构建端到端的网络，对输入图像进行`Transfrom`后，并送入网络模型，预测结果。\n\ndemo的功能浏览完了，接下去的内容会从`Dataloader`开始，构建模型，评估测试，最终结果可视化。","tags":["深度学习","pytorch","源码阅读","目标检测","框架学习"],"categories":["codes"]},{"title":"detectron2 源码阅读系列 01 安装流程","url":"/2020/09/10/20200910-detectron2-01-installation/","content":"\n#### 0. 前言\n[Detectron2](https://github.com/facebookresearch/detectron2) 是Facebook AI Research (FAIR)开源的目标检测框架，在上一代的基础上，基于Pytorch进行了实现。模型涵盖目标检测的Faster R-CNN, RetinaNet, 实例分割的Mask R-CNN, 关键点估计的Dense Pose, 全景分割的Panoptic FPN等多个模型，以及相应数据的加载等。\n\n最近在自己的项目实现过程，深刻意识到设计一个系统与解决一道代码题目是两个不同的概念。正好项目中需要用到Detectron2的部分代码，因此将Detectron2作为自己学习代码规范和项目实现的样例。\n\n<!-- more -->\n\n### 1. 主机环境\n\n| 名称  | 版本  |\n|:-------: | :--------:|\n|操作系统|Linux 16.04|\n|显卡\t|GTX Titan X|\n|驱动\t|418.56\t\t|\n|CUDA|9.0\t\t|\n|Conda|4.84\t\t|\n\n### 2. 安装过程\n逐行运行下列命令即可，按照[官方文档](https://detectron2.readthedocs.io/tutorials/install.html)运行的时候，发现少了`opencv-python`库，因此补了一条。\n\n```\nconda create -n detectron2 python=3.6\nconda activate detectron2\nconda install pytorch torchvision cudatoolkit=9.2 -c pytorch\npip install opencv-python\ngit clone https://github.com/facebookresearch/detectron2.git\npython -m pip install -e detectron2\n```\n\n### 3. Demo测试\n#### 3.1 数据准备\n提前找一张图片放进`demo`文件夹中，这里从[网络](http://www.gudaimi.com/Sjfyltu/201812/99590.html)随机照了一张图片，保存的文件名为 `test_pic.jpg`，结果并保存为同目录下的` res_pic.jpg` (`--output` 后的参数)；如果要测试视频，输出参数要将`--input` 改为`--video-input`。其余参数可以使用`python demo.py -h` 进行查看。\n\n####3.2 代码运行\n进入`demo`文件夹测试\n\n```\ncd dtectron2/demo\npython demo.py --config-file ../configs/COCO- InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml \\\n--input test_pic.jpg --output res_pic.jpg --opts MODEL.WEIGHTS \\\ndetectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\n```\n\n##### 3.2.1 输入\n![输入图像](http://hexotuchuang.oss-cn-beijing.aliyuncs.com/blog_init/post/detectron2/test_pic.jpeg)\n\n##### 3.2.2 结果\n![测试结果](http://hexotuchuang.oss-cn-beijing.aliyuncs.com/blog_init/post/detectron2/res_pic.jpg)\n\n到这里就完成了Detectron2的全部安装过程了，明天开始阅读demo.py文件。","tags":["深度学习","pytorch","源码阅读","目标检测","框架学习"],"categories":["codes"]},{"title":"蝉鸣低语","url":"/2020/06/30/2020060-gossip-001/","content":"\n## 引子\n\n十七年的蛰伏，只为一个盛夏，只为这个盛夏。\n\n无人的深夜，从地下钻出。这并不是第一次看见这个世界，上一次不过是十七年前，出生后的匆匆一瞥。现在，依旧无暇关注，因为还有更重要的事情等着我。爬上树梢，准备开启生命中的最后一个阶段。在原来的身体里挣扎着，感受着躯壳撕裂开来，一点一点的剥离开来。相同的事情，已经经历了四次，现如今看着褪下的身躯，感叹着这是最后一次了，终于到最后一次了，梦想化作双翼。至此，总算拥有了生命，与灵魂契合的生命。\n\n<!-- more -->\n\n## 缘起\n\n自己的身份证快要过期了，下一张的有效期便是二十年。大哥自成年之后，即将走上奔三的生涯。终于是到了这个失去的年纪，不论是自己的亲人，抑或是崇拜的偶像，从霍金到张守晟，从金庸到斯坦·李，那些为人类科学事业不断奉献的人，那些伴随着自己成长的角色，终究都成了过往。\n\n自从大学毕业之后，时间过得飞快。不记得哪一天开始，不怎么找歌听了，觉得现在列表里的就不错。也追不上新番了，仿佛中二病，黑子的篮球还是去年的番，夏目友人帐第五季，也不过是刚出的消息。最近买来的游戏，静静的躺在那里，看完了游戏启动画面，就再也没有然后。或许只是出于情怀喜+1吧，半开玩笑的说着“自己花钱买的游戏，凭什么还要花时间玩”，心里却多少有些落寞。遇到事情，不论好坏，也不愿去多谈什么，只是默默放在心底。\n\n回首过去，才发觉忙忙碌碌，却什么都没有做。学了很多，却又什么都没有记住。人生实苦，于我而言，生命本身是没有意义的，更多的是寻找意义的过程，以及这一生中，不论悲喜，给自己留下的感觉。\n\n未来，我如果有一个孩子，我不指望他能达成多大的成就，不求大富大贵，不求功名显赫。但是我想在灿烂星河下讲着牛郎织女的故事，说着奥林匹斯山上众神的纷争，告诉他阿基米德撬动地球的支点，以及梵高笔下的星夜。希望他能够领略浩瀚宇宙中的美景，更希望他能够被那一段段深情的文字所打动，为科学的公式所折服，为艺术的升华所感染。在万家灯火中，体验人间冷暖，学会感恩，懂得体谅。终于，他觉得无愧此生，平静坦然地按自己的计划过完余生。\n\n可是孩子终究是独立的个体，本不应该按照我设定的路线走下去。这个世界，有太多的不如意，带他来到这个世界，或许就已经亏欠他太多了。直到朋友说起，这可能也是你想成为的人吧。那一刻，才意识到，原来这是自己想要成为的样子啊。自己终究不能免俗，将自己未完成的事情，寄希望于自己的孩子，更可笑的是自己还不自知。又或许是自己早早的框定了自己的余生，已经没有去戍守边疆，保家卫国，若是用尽全力，去做一些事情，做成一些事情，就很好了。\n\n关于余生，我总需要一个出口。筹划了很久的博客，总算在今年提上了日程，记下人间的美好，存下自己的呢喃，也为自己的盛夏蛰伏。故此，蝉鸣。","tags":["人间值得"],"categories":["thoughts"]}]